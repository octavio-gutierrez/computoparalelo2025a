{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Workaround"
      ],
      "metadata": {
        "id": "aaayjVhzuVAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba-cuda==0.4.0\n",
        "from numba import config\n",
        "\n",
        "config.CUDA_ENABLE_PYNVJITLINK = 1"
      ],
      "metadata": {
        "id": "3DJX4BOeuT8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerías y detección de GPUs"
      ],
      "metadata": {
        "id": "dNfb9mWeA9BY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTC-hc4RAcGq",
        "outputId": "c3295df8-2bcb-4f47-b017-5c9b8fe8a6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 CUDA devices\n",
            "id 0             b'Tesla T4'                              [SUPPORTED]\n",
            "                      Compute Capability: 7.5\n",
            "                           PCI Device ID: 4\n",
            "                              PCI Bus ID: 0\n",
            "                                    UUID: GPU-4b6579a9-5126-3e40-218c-23ec2f2d2ac0\n",
            "                                Watchdog: Disabled\n",
            "             FP32/FP64 Performance Ratio: 32\n",
            "Summary:\n",
            "\t1/1 devices are supported\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<weakproxy at 0x7a9efe9f27a0 to Device at 0x7a9efe9cc880>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "import numba\n",
        "import math\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import data, io\n",
        "import requests\n",
        "from time import perf_counter\n",
        "\n",
        "cuda.gpus\n",
        "cuda.detect()\n",
        "cuda.select_device(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cantidad de SMs y Cores"
      ],
      "metadata": {
        "id": "00Wc-3NcBJqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cc_cores_per_SM_dict = {  (2,0) : 32,  (2,1) : 48, (3,0) : 192, (3,5) : 192, (3,7) : 192, (5,0) : 128, (5,2) : 128, (6,0) : 64, (6,1) : 128, (7,0) : 64, (7,5) : 64, (8,0) : 64, (8,6) : 128, (8,9) : 128, (9,0) : 128 }\n",
        "device = cuda.get_current_device()\n",
        "sms = getattr(device, \"MULTIPROCESSOR_COUNT\")\n",
        "cc = device.compute_capability\n",
        "cores_per_sm = cc_cores_per_SM_dict.get(cc)\n",
        "num_cores = cores_per_sm * sms\n",
        "print(\"GPU compute capability: \", cc)\n",
        "print(\"Número de SM: \", sms)\n",
        "print(\"Número de cores: \", num_cores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKok32mdAgtq",
        "outputId": "761bc3a2-237a-430c-a883-6699d603436c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU compute capability:  (7, 5)\n",
            "Número de SM:  40\n",
            "Número de cores:  2560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atributos relevantes del GPU"
      ],
      "metadata": {
        "id": "rjHWVuy-BMlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba.cuda.cudadrv import enums\n",
        "\n",
        "device = cuda.get_current_device()\n",
        "attribs = [name.replace(\"CU_DEVICE_ATTRIBUTE_\", \"\") for name in dir(enums) if name.startswith(\"CU_DEVICE_ATTRIBUTE_\")]\n",
        "for attr in attribs:\n",
        "    print(attr, \"=\", getattr(device, attr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va14dIxFAjWD",
        "outputId": "543cba2d-e46f-4ced-ece3-f0b83b69c581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASYNC_ENGINE_COUNT = 3\n",
            "CAN_MAP_HOST_MEMORY = 1\n",
            "CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM = 1\n",
            "CLOCK_RATE = 1590000\n",
            "COMPUTE_CAPABILITY_MAJOR = 7\n",
            "COMPUTE_CAPABILITY_MINOR = 5\n",
            "COMPUTE_MODE = 0\n",
            "COMPUTE_PREEMPTION_SUPPORTED = 1\n",
            "CONCURRENT_KERNELS = 1\n",
            "CONCURRENT_MANAGED_ACCESS = 1\n",
            "COOPERATIVE_LAUNCH = 1\n",
            "COOPERATIVE_MULTI_DEVICE_LAUNCH = 1\n",
            "ECC_ENABLED = 1\n",
            "GLOBAL_L1_CACHE_SUPPORTED = 1\n",
            "GLOBAL_MEMORY_BUS_WIDTH = 256\n",
            "GPU_OVERLAP = 1\n",
            "HOST_NATIVE_ATOMIC_SUPPORTED = 0\n",
            "INTEGRATED = 0\n",
            "IS_MULTI_GPU_BOARD = 0\n",
            "KERNEL_EXEC_TIMEOUT = 0\n",
            "L2_CACHE_SIZE = 4194304\n",
            "LOCAL_L1_CACHE_SUPPORTED = 1\n",
            "MANAGED_MEMORY = 1\n",
            "MAX_BLOCK_DIM_X = 1024\n",
            "MAX_BLOCK_DIM_Y = 1024\n",
            "MAX_BLOCK_DIM_Z = 64\n",
            "MAX_GRID_DIM_X = 2147483647\n",
            "MAX_GRID_DIM_Y = 65535\n",
            "MAX_GRID_DIM_Z = 65535\n",
            "MAX_MAX_TEXTURE_2D_MIPMAPPED_HEIGHT = 32768\n",
            "MAX_PITCH = 2147483647\n",
            "MAX_REGISTERS_PER_BLOCK = 65536\n",
            "MAX_REGISTERS_PER_MULTIPROCESSOR = 65536\n",
            "MAX_SHARED_MEMORY_PER_BLOCK = 49152\n",
            "MAX_SHARED_MEMORY_PER_BLOCK_OPTIN = 65536\n",
            "MAX_SHARED_MEMORY_PER_MULTIPROCESSOR = 65536\n",
            "MAX_SURFACE_1D_LAYERED_LAYERS = 2048\n",
            "MAX_SURFACE_1D_LAYERED_WIDTH = 32768\n",
            "MAX_SURFACE_1D_WIDTH = 32768\n",
            "MAX_SURFACE_2D_HEIGHT = 65536\n",
            "MAX_SURFACE_2D_LAYERED_HEIGHT = 32768\n",
            "MAX_SURFACE_2D_LAYERED_LAYERS = 2048\n",
            "MAX_SURFACE_2D_LAYERED_WIDTH = 32768\n",
            "MAX_SURFACE_2D_WIDTH = 131072\n",
            "MAX_SURFACE_3D_DEPTH = 16384\n",
            "MAX_SURFACE_3D_HEIGHT = 16384\n",
            "MAX_SURFACE_3D_WIDTH = 16384\n",
            "MAX_SURFACE_CUBEMAP_LAYERED_LAYERS = 2046\n",
            "MAX_SURFACE_CUBEMAP_LAYERED_WIDTH = 32768\n",
            "MAX_SURFACE_CUBEMAP_WIDTH = 32768\n",
            "MAX_TEXTURE_1D_LAYERED_LAYERS = 2048\n",
            "MAX_TEXTURE_1D_LAYERED_WIDTH = 32768\n",
            "MAX_TEXTURE_1D_LINEAR_WIDTH = 268435456\n",
            "MAX_TEXTURE_1D_MIPMAPPED_WIDTH = 32768\n",
            "MAX_TEXTURE_1D_WIDTH = 131072\n",
            "MAX_TEXTURE_2D_GATHER_HEIGHT = 32768\n",
            "MAX_TEXTURE_2D_GATHER_WIDTH = 32768\n",
            "MAX_TEXTURE_2D_HEIGHT = 65536\n",
            "MAX_TEXTURE_2D_LAYERED_HEIGHT = 32768\n",
            "MAX_TEXTURE_2D_LAYERED_LAYERS = 2048\n",
            "MAX_TEXTURE_2D_LAYERED_WIDTH = 32768\n",
            "MAX_TEXTURE_2D_LINEAR_HEIGHT = 65000\n",
            "MAX_TEXTURE_2D_LINEAR_PITCH = 2097120\n",
            "MAX_TEXTURE_2D_LINEAR_WIDTH = 131072\n",
            "MAX_TEXTURE_2D_MIPMAPPED_WIDTH = 32768\n",
            "MAX_TEXTURE_2D_WIDTH = 131072\n",
            "MAX_TEXTURE_3D_DEPTH = 16384\n",
            "MAX_TEXTURE_3D_DEPTH_ALT = 32768\n",
            "MAX_TEXTURE_3D_HEIGHT = 16384\n",
            "MAX_TEXTURE_3D_HEIGHT_ALT = 8192\n",
            "MAX_TEXTURE_3D_WIDTH = 16384\n",
            "MAX_TEXTURE_3D_WIDTH_ALT = 8192\n",
            "MAX_TEXTURE_CUBEMAP_LAYERED_LAYERS = 2046\n",
            "MAX_TEXTURE_CUBEMAP_LAYERED_WIDTH = 32768\n",
            "MAX_TEXTURE_CUBEMAP_WIDTH = 32768\n",
            "MAX_THREADS_PER_BLOCK = 1024\n",
            "MAX_THREADS_PER_MULTI_PROCESSOR = 1024\n",
            "MEMORY_CLOCK_RATE = 5001000\n",
            "MULTIPROCESSOR_COUNT = 40\n",
            "MULTI_GPU_BOARD_GROUP_ID = 0\n",
            "PAGEABLE_MEMORY_ACCESS = 0\n",
            "PCI_BUS_ID = 0\n",
            "PCI_DEVICE_ID = 4\n",
            "PCI_DOMAIN_ID = 0\n",
            "SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO = 32\n",
            "STREAM_PRIORITIES_SUPPORTED = 1\n",
            "SURFACE_ALIGNMENT = 512\n",
            "TCC_DRIVER = 0\n",
            "TEXTURE_ALIGNMENT = 512\n",
            "TEXTURE_PITCH_ALIGNMENT = 32\n",
            "TOTAL_CONSTANT_MEMORY = 65536\n",
            "UNIFIED_ADDRESSING = 1\n",
            "WARP_SIZE = 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lanzamiento de un Kernel simplista para un Vector Ad-hoc"
      ],
      "metadata": {
        "id": "n_d9s5AABSl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def a_kernel(my_array):\n",
        "  \"\"\"\n",
        "  Aquí iría el código de mi kernel\n",
        "  \"\"\"\n",
        "  pass\n",
        "\n",
        "my_array = np.ones(320)\n",
        "# hilos por bloque debe estar en función del warp size\n",
        "threads_per_block = 32\n",
        "# número de bloques en la rejilla\n",
        "my_grid  = my_array.size // threads_per_block\n",
        "a_kernel[my_grid, threads_per_block](my_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j3LM0GqAk96",
        "outputId": "07a0d7c4-b2c3-4f06-9d79-d8174905bda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 10 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Posición de los Hilos relativa a los Datos para un Vector Genérico y copiar vectores al Device"
      ],
      "metadata": {
        "id": "TTfQEPVgBXLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def multiplier_kernel(my_array, structure):\n",
        "  position = cuda.threadIdx.x + cuda.blockIdx.x*cuda.blockDim.x\n",
        "  if position < my_array.size:\n",
        "    my_array[position] *= 2\n",
        "    structure[position][0] = position\n",
        "    structure[position][1] = cuda.threadIdx.x\n",
        "    structure[position][2] = cuda.blockIdx.x\n",
        "    structure[position][3] = cuda.blockDim.x\n",
        "\n",
        "VECTOR_SIZE = 321\n",
        "my_array = np.ones(VECTOR_SIZE)\n",
        "# Copiar arreglo al device\n",
        "device_array = cuda.device_array_like(my_array)\n",
        "\n",
        "VARS_TO_DEBUG = 4;\n",
        "structure = np.zeros(shape=(VECTOR_SIZE, VARS_TO_DEBUG))\n",
        "\n",
        "threads_per_block = 32\n",
        "\n",
        "# número de bloques en la rejilla\n",
        "my_grid = math.ceil(my_array.size / threads_per_block)\n",
        "my_grid\n",
        "\n",
        "multiplier_kernel[my_grid, threads_per_block](my_array, structure)\n",
        "print(my_array)\n",
        "for step in structure:\n",
        "  print(step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Au1AJ2cAnzS",
        "outputId": "a8899ed4-bd44-4c15-8779-94ddeb9d707f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 11 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            "[ 0.  0.  0. 32.]\n",
            "[ 1.  1.  0. 32.]\n",
            "[ 2.  2.  0. 32.]\n",
            "[ 3.  3.  0. 32.]\n",
            "[ 4.  4.  0. 32.]\n",
            "[ 5.  5.  0. 32.]\n",
            "[ 6.  6.  0. 32.]\n",
            "[ 7.  7.  0. 32.]\n",
            "[ 8.  8.  0. 32.]\n",
            "[ 9.  9.  0. 32.]\n",
            "[10. 10.  0. 32.]\n",
            "[11. 11.  0. 32.]\n",
            "[12. 12.  0. 32.]\n",
            "[13. 13.  0. 32.]\n",
            "[14. 14.  0. 32.]\n",
            "[15. 15.  0. 32.]\n",
            "[16. 16.  0. 32.]\n",
            "[17. 17.  0. 32.]\n",
            "[18. 18.  0. 32.]\n",
            "[19. 19.  0. 32.]\n",
            "[20. 20.  0. 32.]\n",
            "[21. 21.  0. 32.]\n",
            "[22. 22.  0. 32.]\n",
            "[23. 23.  0. 32.]\n",
            "[24. 24.  0. 32.]\n",
            "[25. 25.  0. 32.]\n",
            "[26. 26.  0. 32.]\n",
            "[27. 27.  0. 32.]\n",
            "[28. 28.  0. 32.]\n",
            "[29. 29.  0. 32.]\n",
            "[30. 30.  0. 32.]\n",
            "[31. 31.  0. 32.]\n",
            "[32.  0.  1. 32.]\n",
            "[33.  1.  1. 32.]\n",
            "[34.  2.  1. 32.]\n",
            "[35.  3.  1. 32.]\n",
            "[36.  4.  1. 32.]\n",
            "[37.  5.  1. 32.]\n",
            "[38.  6.  1. 32.]\n",
            "[39.  7.  1. 32.]\n",
            "[40.  8.  1. 32.]\n",
            "[41.  9.  1. 32.]\n",
            "[42. 10.  1. 32.]\n",
            "[43. 11.  1. 32.]\n",
            "[44. 12.  1. 32.]\n",
            "[45. 13.  1. 32.]\n",
            "[46. 14.  1. 32.]\n",
            "[47. 15.  1. 32.]\n",
            "[48. 16.  1. 32.]\n",
            "[49. 17.  1. 32.]\n",
            "[50. 18.  1. 32.]\n",
            "[51. 19.  1. 32.]\n",
            "[52. 20.  1. 32.]\n",
            "[53. 21.  1. 32.]\n",
            "[54. 22.  1. 32.]\n",
            "[55. 23.  1. 32.]\n",
            "[56. 24.  1. 32.]\n",
            "[57. 25.  1. 32.]\n",
            "[58. 26.  1. 32.]\n",
            "[59. 27.  1. 32.]\n",
            "[60. 28.  1. 32.]\n",
            "[61. 29.  1. 32.]\n",
            "[62. 30.  1. 32.]\n",
            "[63. 31.  1. 32.]\n",
            "[64.  0.  2. 32.]\n",
            "[65.  1.  2. 32.]\n",
            "[66.  2.  2. 32.]\n",
            "[67.  3.  2. 32.]\n",
            "[68.  4.  2. 32.]\n",
            "[69.  5.  2. 32.]\n",
            "[70.  6.  2. 32.]\n",
            "[71.  7.  2. 32.]\n",
            "[72.  8.  2. 32.]\n",
            "[73.  9.  2. 32.]\n",
            "[74. 10.  2. 32.]\n",
            "[75. 11.  2. 32.]\n",
            "[76. 12.  2. 32.]\n",
            "[77. 13.  2. 32.]\n",
            "[78. 14.  2. 32.]\n",
            "[79. 15.  2. 32.]\n",
            "[80. 16.  2. 32.]\n",
            "[81. 17.  2. 32.]\n",
            "[82. 18.  2. 32.]\n",
            "[83. 19.  2. 32.]\n",
            "[84. 20.  2. 32.]\n",
            "[85. 21.  2. 32.]\n",
            "[86. 22.  2. 32.]\n",
            "[87. 23.  2. 32.]\n",
            "[88. 24.  2. 32.]\n",
            "[89. 25.  2. 32.]\n",
            "[90. 26.  2. 32.]\n",
            "[91. 27.  2. 32.]\n",
            "[92. 28.  2. 32.]\n",
            "[93. 29.  2. 32.]\n",
            "[94. 30.  2. 32.]\n",
            "[95. 31.  2. 32.]\n",
            "[96.  0.  3. 32.]\n",
            "[97.  1.  3. 32.]\n",
            "[98.  2.  3. 32.]\n",
            "[99.  3.  3. 32.]\n",
            "[100.   4.   3.  32.]\n",
            "[101.   5.   3.  32.]\n",
            "[102.   6.   3.  32.]\n",
            "[103.   7.   3.  32.]\n",
            "[104.   8.   3.  32.]\n",
            "[105.   9.   3.  32.]\n",
            "[106.  10.   3.  32.]\n",
            "[107.  11.   3.  32.]\n",
            "[108.  12.   3.  32.]\n",
            "[109.  13.   3.  32.]\n",
            "[110.  14.   3.  32.]\n",
            "[111.  15.   3.  32.]\n",
            "[112.  16.   3.  32.]\n",
            "[113.  17.   3.  32.]\n",
            "[114.  18.   3.  32.]\n",
            "[115.  19.   3.  32.]\n",
            "[116.  20.   3.  32.]\n",
            "[117.  21.   3.  32.]\n",
            "[118.  22.   3.  32.]\n",
            "[119.  23.   3.  32.]\n",
            "[120.  24.   3.  32.]\n",
            "[121.  25.   3.  32.]\n",
            "[122.  26.   3.  32.]\n",
            "[123.  27.   3.  32.]\n",
            "[124.  28.   3.  32.]\n",
            "[125.  29.   3.  32.]\n",
            "[126.  30.   3.  32.]\n",
            "[127.  31.   3.  32.]\n",
            "[128.   0.   4.  32.]\n",
            "[129.   1.   4.  32.]\n",
            "[130.   2.   4.  32.]\n",
            "[131.   3.   4.  32.]\n",
            "[132.   4.   4.  32.]\n",
            "[133.   5.   4.  32.]\n",
            "[134.   6.   4.  32.]\n",
            "[135.   7.   4.  32.]\n",
            "[136.   8.   4.  32.]\n",
            "[137.   9.   4.  32.]\n",
            "[138.  10.   4.  32.]\n",
            "[139.  11.   4.  32.]\n",
            "[140.  12.   4.  32.]\n",
            "[141.  13.   4.  32.]\n",
            "[142.  14.   4.  32.]\n",
            "[143.  15.   4.  32.]\n",
            "[144.  16.   4.  32.]\n",
            "[145.  17.   4.  32.]\n",
            "[146.  18.   4.  32.]\n",
            "[147.  19.   4.  32.]\n",
            "[148.  20.   4.  32.]\n",
            "[149.  21.   4.  32.]\n",
            "[150.  22.   4.  32.]\n",
            "[151.  23.   4.  32.]\n",
            "[152.  24.   4.  32.]\n",
            "[153.  25.   4.  32.]\n",
            "[154.  26.   4.  32.]\n",
            "[155.  27.   4.  32.]\n",
            "[156.  28.   4.  32.]\n",
            "[157.  29.   4.  32.]\n",
            "[158.  30.   4.  32.]\n",
            "[159.  31.   4.  32.]\n",
            "[160.   0.   5.  32.]\n",
            "[161.   1.   5.  32.]\n",
            "[162.   2.   5.  32.]\n",
            "[163.   3.   5.  32.]\n",
            "[164.   4.   5.  32.]\n",
            "[165.   5.   5.  32.]\n",
            "[166.   6.   5.  32.]\n",
            "[167.   7.   5.  32.]\n",
            "[168.   8.   5.  32.]\n",
            "[169.   9.   5.  32.]\n",
            "[170.  10.   5.  32.]\n",
            "[171.  11.   5.  32.]\n",
            "[172.  12.   5.  32.]\n",
            "[173.  13.   5.  32.]\n",
            "[174.  14.   5.  32.]\n",
            "[175.  15.   5.  32.]\n",
            "[176.  16.   5.  32.]\n",
            "[177.  17.   5.  32.]\n",
            "[178.  18.   5.  32.]\n",
            "[179.  19.   5.  32.]\n",
            "[180.  20.   5.  32.]\n",
            "[181.  21.   5.  32.]\n",
            "[182.  22.   5.  32.]\n",
            "[183.  23.   5.  32.]\n",
            "[184.  24.   5.  32.]\n",
            "[185.  25.   5.  32.]\n",
            "[186.  26.   5.  32.]\n",
            "[187.  27.   5.  32.]\n",
            "[188.  28.   5.  32.]\n",
            "[189.  29.   5.  32.]\n",
            "[190.  30.   5.  32.]\n",
            "[191.  31.   5.  32.]\n",
            "[192.   0.   6.  32.]\n",
            "[193.   1.   6.  32.]\n",
            "[194.   2.   6.  32.]\n",
            "[195.   3.   6.  32.]\n",
            "[196.   4.   6.  32.]\n",
            "[197.   5.   6.  32.]\n",
            "[198.   6.   6.  32.]\n",
            "[199.   7.   6.  32.]\n",
            "[200.   8.   6.  32.]\n",
            "[201.   9.   6.  32.]\n",
            "[202.  10.   6.  32.]\n",
            "[203.  11.   6.  32.]\n",
            "[204.  12.   6.  32.]\n",
            "[205.  13.   6.  32.]\n",
            "[206.  14.   6.  32.]\n",
            "[207.  15.   6.  32.]\n",
            "[208.  16.   6.  32.]\n",
            "[209.  17.   6.  32.]\n",
            "[210.  18.   6.  32.]\n",
            "[211.  19.   6.  32.]\n",
            "[212.  20.   6.  32.]\n",
            "[213.  21.   6.  32.]\n",
            "[214.  22.   6.  32.]\n",
            "[215.  23.   6.  32.]\n",
            "[216.  24.   6.  32.]\n",
            "[217.  25.   6.  32.]\n",
            "[218.  26.   6.  32.]\n",
            "[219.  27.   6.  32.]\n",
            "[220.  28.   6.  32.]\n",
            "[221.  29.   6.  32.]\n",
            "[222.  30.   6.  32.]\n",
            "[223.  31.   6.  32.]\n",
            "[224.   0.   7.  32.]\n",
            "[225.   1.   7.  32.]\n",
            "[226.   2.   7.  32.]\n",
            "[227.   3.   7.  32.]\n",
            "[228.   4.   7.  32.]\n",
            "[229.   5.   7.  32.]\n",
            "[230.   6.   7.  32.]\n",
            "[231.   7.   7.  32.]\n",
            "[232.   8.   7.  32.]\n",
            "[233.   9.   7.  32.]\n",
            "[234.  10.   7.  32.]\n",
            "[235.  11.   7.  32.]\n",
            "[236.  12.   7.  32.]\n",
            "[237.  13.   7.  32.]\n",
            "[238.  14.   7.  32.]\n",
            "[239.  15.   7.  32.]\n",
            "[240.  16.   7.  32.]\n",
            "[241.  17.   7.  32.]\n",
            "[242.  18.   7.  32.]\n",
            "[243.  19.   7.  32.]\n",
            "[244.  20.   7.  32.]\n",
            "[245.  21.   7.  32.]\n",
            "[246.  22.   7.  32.]\n",
            "[247.  23.   7.  32.]\n",
            "[248.  24.   7.  32.]\n",
            "[249.  25.   7.  32.]\n",
            "[250.  26.   7.  32.]\n",
            "[251.  27.   7.  32.]\n",
            "[252.  28.   7.  32.]\n",
            "[253.  29.   7.  32.]\n",
            "[254.  30.   7.  32.]\n",
            "[255.  31.   7.  32.]\n",
            "[256.   0.   8.  32.]\n",
            "[257.   1.   8.  32.]\n",
            "[258.   2.   8.  32.]\n",
            "[259.   3.   8.  32.]\n",
            "[260.   4.   8.  32.]\n",
            "[261.   5.   8.  32.]\n",
            "[262.   6.   8.  32.]\n",
            "[263.   7.   8.  32.]\n",
            "[264.   8.   8.  32.]\n",
            "[265.   9.   8.  32.]\n",
            "[266.  10.   8.  32.]\n",
            "[267.  11.   8.  32.]\n",
            "[268.  12.   8.  32.]\n",
            "[269.  13.   8.  32.]\n",
            "[270.  14.   8.  32.]\n",
            "[271.  15.   8.  32.]\n",
            "[272.  16.   8.  32.]\n",
            "[273.  17.   8.  32.]\n",
            "[274.  18.   8.  32.]\n",
            "[275.  19.   8.  32.]\n",
            "[276.  20.   8.  32.]\n",
            "[277.  21.   8.  32.]\n",
            "[278.  22.   8.  32.]\n",
            "[279.  23.   8.  32.]\n",
            "[280.  24.   8.  32.]\n",
            "[281.  25.   8.  32.]\n",
            "[282.  26.   8.  32.]\n",
            "[283.  27.   8.  32.]\n",
            "[284.  28.   8.  32.]\n",
            "[285.  29.   8.  32.]\n",
            "[286.  30.   8.  32.]\n",
            "[287.  31.   8.  32.]\n",
            "[288.   0.   9.  32.]\n",
            "[289.   1.   9.  32.]\n",
            "[290.   2.   9.  32.]\n",
            "[291.   3.   9.  32.]\n",
            "[292.   4.   9.  32.]\n",
            "[293.   5.   9.  32.]\n",
            "[294.   6.   9.  32.]\n",
            "[295.   7.   9.  32.]\n",
            "[296.   8.   9.  32.]\n",
            "[297.   9.   9.  32.]\n",
            "[298.  10.   9.  32.]\n",
            "[299.  11.   9.  32.]\n",
            "[300.  12.   9.  32.]\n",
            "[301.  13.   9.  32.]\n",
            "[302.  14.   9.  32.]\n",
            "[303.  15.   9.  32.]\n",
            "[304.  16.   9.  32.]\n",
            "[305.  17.   9.  32.]\n",
            "[306.  18.   9.  32.]\n",
            "[307.  19.   9.  32.]\n",
            "[308.  20.   9.  32.]\n",
            "[309.  21.   9.  32.]\n",
            "[310.  22.   9.  32.]\n",
            "[311.  23.   9.  32.]\n",
            "[312.  24.   9.  32.]\n",
            "[313.  25.   9.  32.]\n",
            "[314.  26.   9.  32.]\n",
            "[315.  27.   9.  32.]\n",
            "[316.  28.   9.  32.]\n",
            "[317.  29.   9.  32.]\n",
            "[318.  30.   9.  32.]\n",
            "[319.  31.   9.  32.]\n",
            "[320.   0.  10.  32.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cálculo rápido de la posición del hilo & Debugging de un programa en CUDA"
      ],
      "metadata": {
        "id": "Q64b0leeBlUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def multiplier_kernel(my_array, structure):\n",
        "  position = cuda.threadIdx.x + cuda.blockIdx.x*cuda.blockDim.x\n",
        "  if position < my_array.size:\n",
        "    my_array[position] *= 2\n",
        "    structure[position][0] = position\n",
        "    structure[position][1] = cuda.threadIdx.x\n",
        "    structure[position][2] = cuda.blockIdx.x\n",
        "    structure[position][3] = cuda.blockDim.x\n",
        "\n",
        "my_array = np.ones(321)\n",
        "device_array = cuda.device_array_like(my_array)\n",
        "structure = np.zeros(shape=(321, 4))\n",
        "threads_per_block = 32\n",
        "my_grid = math.ceil(my_array.size / threads_per_block)\n",
        "multiplier_kernel[my_grid, threads_per_block](my_array, structure)\n",
        "print(my_array)\n",
        "for step in structure:\n",
        "  print(step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt4aZVMGBgTr",
        "outputId": "b5acea07-bf15-421a-935b-8efb8fa3e195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 11 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:886: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
            "[ 0.  0.  0. 32.]\n",
            "[ 1.  1.  0. 32.]\n",
            "[ 2.  2.  0. 32.]\n",
            "[ 3.  3.  0. 32.]\n",
            "[ 4.  4.  0. 32.]\n",
            "[ 5.  5.  0. 32.]\n",
            "[ 6.  6.  0. 32.]\n",
            "[ 7.  7.  0. 32.]\n",
            "[ 8.  8.  0. 32.]\n",
            "[ 9.  9.  0. 32.]\n",
            "[10. 10.  0. 32.]\n",
            "[11. 11.  0. 32.]\n",
            "[12. 12.  0. 32.]\n",
            "[13. 13.  0. 32.]\n",
            "[14. 14.  0. 32.]\n",
            "[15. 15.  0. 32.]\n",
            "[16. 16.  0. 32.]\n",
            "[17. 17.  0. 32.]\n",
            "[18. 18.  0. 32.]\n",
            "[19. 19.  0. 32.]\n",
            "[20. 20.  0. 32.]\n",
            "[21. 21.  0. 32.]\n",
            "[22. 22.  0. 32.]\n",
            "[23. 23.  0. 32.]\n",
            "[24. 24.  0. 32.]\n",
            "[25. 25.  0. 32.]\n",
            "[26. 26.  0. 32.]\n",
            "[27. 27.  0. 32.]\n",
            "[28. 28.  0. 32.]\n",
            "[29. 29.  0. 32.]\n",
            "[30. 30.  0. 32.]\n",
            "[31. 31.  0. 32.]\n",
            "[32.  0.  1. 32.]\n",
            "[33.  1.  1. 32.]\n",
            "[34.  2.  1. 32.]\n",
            "[35.  3.  1. 32.]\n",
            "[36.  4.  1. 32.]\n",
            "[37.  5.  1. 32.]\n",
            "[38.  6.  1. 32.]\n",
            "[39.  7.  1. 32.]\n",
            "[40.  8.  1. 32.]\n",
            "[41.  9.  1. 32.]\n",
            "[42. 10.  1. 32.]\n",
            "[43. 11.  1. 32.]\n",
            "[44. 12.  1. 32.]\n",
            "[45. 13.  1. 32.]\n",
            "[46. 14.  1. 32.]\n",
            "[47. 15.  1. 32.]\n",
            "[48. 16.  1. 32.]\n",
            "[49. 17.  1. 32.]\n",
            "[50. 18.  1. 32.]\n",
            "[51. 19.  1. 32.]\n",
            "[52. 20.  1. 32.]\n",
            "[53. 21.  1. 32.]\n",
            "[54. 22.  1. 32.]\n",
            "[55. 23.  1. 32.]\n",
            "[56. 24.  1. 32.]\n",
            "[57. 25.  1. 32.]\n",
            "[58. 26.  1. 32.]\n",
            "[59. 27.  1. 32.]\n",
            "[60. 28.  1. 32.]\n",
            "[61. 29.  1. 32.]\n",
            "[62. 30.  1. 32.]\n",
            "[63. 31.  1. 32.]\n",
            "[64.  0.  2. 32.]\n",
            "[65.  1.  2. 32.]\n",
            "[66.  2.  2. 32.]\n",
            "[67.  3.  2. 32.]\n",
            "[68.  4.  2. 32.]\n",
            "[69.  5.  2. 32.]\n",
            "[70.  6.  2. 32.]\n",
            "[71.  7.  2. 32.]\n",
            "[72.  8.  2. 32.]\n",
            "[73.  9.  2. 32.]\n",
            "[74. 10.  2. 32.]\n",
            "[75. 11.  2. 32.]\n",
            "[76. 12.  2. 32.]\n",
            "[77. 13.  2. 32.]\n",
            "[78. 14.  2. 32.]\n",
            "[79. 15.  2. 32.]\n",
            "[80. 16.  2. 32.]\n",
            "[81. 17.  2. 32.]\n",
            "[82. 18.  2. 32.]\n",
            "[83. 19.  2. 32.]\n",
            "[84. 20.  2. 32.]\n",
            "[85. 21.  2. 32.]\n",
            "[86. 22.  2. 32.]\n",
            "[87. 23.  2. 32.]\n",
            "[88. 24.  2. 32.]\n",
            "[89. 25.  2. 32.]\n",
            "[90. 26.  2. 32.]\n",
            "[91. 27.  2. 32.]\n",
            "[92. 28.  2. 32.]\n",
            "[93. 29.  2. 32.]\n",
            "[94. 30.  2. 32.]\n",
            "[95. 31.  2. 32.]\n",
            "[96.  0.  3. 32.]\n",
            "[97.  1.  3. 32.]\n",
            "[98.  2.  3. 32.]\n",
            "[99.  3.  3. 32.]\n",
            "[100.   4.   3.  32.]\n",
            "[101.   5.   3.  32.]\n",
            "[102.   6.   3.  32.]\n",
            "[103.   7.   3.  32.]\n",
            "[104.   8.   3.  32.]\n",
            "[105.   9.   3.  32.]\n",
            "[106.  10.   3.  32.]\n",
            "[107.  11.   3.  32.]\n",
            "[108.  12.   3.  32.]\n",
            "[109.  13.   3.  32.]\n",
            "[110.  14.   3.  32.]\n",
            "[111.  15.   3.  32.]\n",
            "[112.  16.   3.  32.]\n",
            "[113.  17.   3.  32.]\n",
            "[114.  18.   3.  32.]\n",
            "[115.  19.   3.  32.]\n",
            "[116.  20.   3.  32.]\n",
            "[117.  21.   3.  32.]\n",
            "[118.  22.   3.  32.]\n",
            "[119.  23.   3.  32.]\n",
            "[120.  24.   3.  32.]\n",
            "[121.  25.   3.  32.]\n",
            "[122.  26.   3.  32.]\n",
            "[123.  27.   3.  32.]\n",
            "[124.  28.   3.  32.]\n",
            "[125.  29.   3.  32.]\n",
            "[126.  30.   3.  32.]\n",
            "[127.  31.   3.  32.]\n",
            "[128.   0.   4.  32.]\n",
            "[129.   1.   4.  32.]\n",
            "[130.   2.   4.  32.]\n",
            "[131.   3.   4.  32.]\n",
            "[132.   4.   4.  32.]\n",
            "[133.   5.   4.  32.]\n",
            "[134.   6.   4.  32.]\n",
            "[135.   7.   4.  32.]\n",
            "[136.   8.   4.  32.]\n",
            "[137.   9.   4.  32.]\n",
            "[138.  10.   4.  32.]\n",
            "[139.  11.   4.  32.]\n",
            "[140.  12.   4.  32.]\n",
            "[141.  13.   4.  32.]\n",
            "[142.  14.   4.  32.]\n",
            "[143.  15.   4.  32.]\n",
            "[144.  16.   4.  32.]\n",
            "[145.  17.   4.  32.]\n",
            "[146.  18.   4.  32.]\n",
            "[147.  19.   4.  32.]\n",
            "[148.  20.   4.  32.]\n",
            "[149.  21.   4.  32.]\n",
            "[150.  22.   4.  32.]\n",
            "[151.  23.   4.  32.]\n",
            "[152.  24.   4.  32.]\n",
            "[153.  25.   4.  32.]\n",
            "[154.  26.   4.  32.]\n",
            "[155.  27.   4.  32.]\n",
            "[156.  28.   4.  32.]\n",
            "[157.  29.   4.  32.]\n",
            "[158.  30.   4.  32.]\n",
            "[159.  31.   4.  32.]\n",
            "[160.   0.   5.  32.]\n",
            "[161.   1.   5.  32.]\n",
            "[162.   2.   5.  32.]\n",
            "[163.   3.   5.  32.]\n",
            "[164.   4.   5.  32.]\n",
            "[165.   5.   5.  32.]\n",
            "[166.   6.   5.  32.]\n",
            "[167.   7.   5.  32.]\n",
            "[168.   8.   5.  32.]\n",
            "[169.   9.   5.  32.]\n",
            "[170.  10.   5.  32.]\n",
            "[171.  11.   5.  32.]\n",
            "[172.  12.   5.  32.]\n",
            "[173.  13.   5.  32.]\n",
            "[174.  14.   5.  32.]\n",
            "[175.  15.   5.  32.]\n",
            "[176.  16.   5.  32.]\n",
            "[177.  17.   5.  32.]\n",
            "[178.  18.   5.  32.]\n",
            "[179.  19.   5.  32.]\n",
            "[180.  20.   5.  32.]\n",
            "[181.  21.   5.  32.]\n",
            "[182.  22.   5.  32.]\n",
            "[183.  23.   5.  32.]\n",
            "[184.  24.   5.  32.]\n",
            "[185.  25.   5.  32.]\n",
            "[186.  26.   5.  32.]\n",
            "[187.  27.   5.  32.]\n",
            "[188.  28.   5.  32.]\n",
            "[189.  29.   5.  32.]\n",
            "[190.  30.   5.  32.]\n",
            "[191.  31.   5.  32.]\n",
            "[192.   0.   6.  32.]\n",
            "[193.   1.   6.  32.]\n",
            "[194.   2.   6.  32.]\n",
            "[195.   3.   6.  32.]\n",
            "[196.   4.   6.  32.]\n",
            "[197.   5.   6.  32.]\n",
            "[198.   6.   6.  32.]\n",
            "[199.   7.   6.  32.]\n",
            "[200.   8.   6.  32.]\n",
            "[201.   9.   6.  32.]\n",
            "[202.  10.   6.  32.]\n",
            "[203.  11.   6.  32.]\n",
            "[204.  12.   6.  32.]\n",
            "[205.  13.   6.  32.]\n",
            "[206.  14.   6.  32.]\n",
            "[207.  15.   6.  32.]\n",
            "[208.  16.   6.  32.]\n",
            "[209.  17.   6.  32.]\n",
            "[210.  18.   6.  32.]\n",
            "[211.  19.   6.  32.]\n",
            "[212.  20.   6.  32.]\n",
            "[213.  21.   6.  32.]\n",
            "[214.  22.   6.  32.]\n",
            "[215.  23.   6.  32.]\n",
            "[216.  24.   6.  32.]\n",
            "[217.  25.   6.  32.]\n",
            "[218.  26.   6.  32.]\n",
            "[219.  27.   6.  32.]\n",
            "[220.  28.   6.  32.]\n",
            "[221.  29.   6.  32.]\n",
            "[222.  30.   6.  32.]\n",
            "[223.  31.   6.  32.]\n",
            "[224.   0.   7.  32.]\n",
            "[225.   1.   7.  32.]\n",
            "[226.   2.   7.  32.]\n",
            "[227.   3.   7.  32.]\n",
            "[228.   4.   7.  32.]\n",
            "[229.   5.   7.  32.]\n",
            "[230.   6.   7.  32.]\n",
            "[231.   7.   7.  32.]\n",
            "[232.   8.   7.  32.]\n",
            "[233.   9.   7.  32.]\n",
            "[234.  10.   7.  32.]\n",
            "[235.  11.   7.  32.]\n",
            "[236.  12.   7.  32.]\n",
            "[237.  13.   7.  32.]\n",
            "[238.  14.   7.  32.]\n",
            "[239.  15.   7.  32.]\n",
            "[240.  16.   7.  32.]\n",
            "[241.  17.   7.  32.]\n",
            "[242.  18.   7.  32.]\n",
            "[243.  19.   7.  32.]\n",
            "[244.  20.   7.  32.]\n",
            "[245.  21.   7.  32.]\n",
            "[246.  22.   7.  32.]\n",
            "[247.  23.   7.  32.]\n",
            "[248.  24.   7.  32.]\n",
            "[249.  25.   7.  32.]\n",
            "[250.  26.   7.  32.]\n",
            "[251.  27.   7.  32.]\n",
            "[252.  28.   7.  32.]\n",
            "[253.  29.   7.  32.]\n",
            "[254.  30.   7.  32.]\n",
            "[255.  31.   7.  32.]\n",
            "[256.   0.   8.  32.]\n",
            "[257.   1.   8.  32.]\n",
            "[258.   2.   8.  32.]\n",
            "[259.   3.   8.  32.]\n",
            "[260.   4.   8.  32.]\n",
            "[261.   5.   8.  32.]\n",
            "[262.   6.   8.  32.]\n",
            "[263.   7.   8.  32.]\n",
            "[264.   8.   8.  32.]\n",
            "[265.   9.   8.  32.]\n",
            "[266.  10.   8.  32.]\n",
            "[267.  11.   8.  32.]\n",
            "[268.  12.   8.  32.]\n",
            "[269.  13.   8.  32.]\n",
            "[270.  14.   8.  32.]\n",
            "[271.  15.   8.  32.]\n",
            "[272.  16.   8.  32.]\n",
            "[273.  17.   8.  32.]\n",
            "[274.  18.   8.  32.]\n",
            "[275.  19.   8.  32.]\n",
            "[276.  20.   8.  32.]\n",
            "[277.  21.   8.  32.]\n",
            "[278.  22.   8.  32.]\n",
            "[279.  23.   8.  32.]\n",
            "[280.  24.   8.  32.]\n",
            "[281.  25.   8.  32.]\n",
            "[282.  26.   8.  32.]\n",
            "[283.  27.   8.  32.]\n",
            "[284.  28.   8.  32.]\n",
            "[285.  29.   8.  32.]\n",
            "[286.  30.   8.  32.]\n",
            "[287.  31.   8.  32.]\n",
            "[288.   0.   9.  32.]\n",
            "[289.   1.   9.  32.]\n",
            "[290.   2.   9.  32.]\n",
            "[291.   3.   9.  32.]\n",
            "[292.   4.   9.  32.]\n",
            "[293.   5.   9.  32.]\n",
            "[294.   6.   9.  32.]\n",
            "[295.   7.   9.  32.]\n",
            "[296.   8.   9.  32.]\n",
            "[297.   9.   9.  32.]\n",
            "[298.  10.   9.  32.]\n",
            "[299.  11.   9.  32.]\n",
            "[300.  12.   9.  32.]\n",
            "[301.  13.   9.  32.]\n",
            "[302.  14.   9.  32.]\n",
            "[303.  15.   9.  32.]\n",
            "[304.  16.   9.  32.]\n",
            "[305.  17.   9.  32.]\n",
            "[306.  18.   9.  32.]\n",
            "[307.  19.   9.  32.]\n",
            "[308.  20.   9.  32.]\n",
            "[309.  21.   9.  32.]\n",
            "[310.  22.   9.  32.]\n",
            "[311.  23.   9.  32.]\n",
            "[312.  24.   9.  32.]\n",
            "[313.  25.   9.  32.]\n",
            "[314.  26.   9.  32.]\n",
            "[315.  27.   9.  32.]\n",
            "[316.  28.   9.  32.]\n",
            "[317.  29.   9.  32.]\n",
            "[318.  30.   9.  32.]\n",
            "[319.  31.   9.  32.]\n",
            "[320.   0.  10.  32.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suma de vectores optimizada con copias al device"
      ],
      "metadata": {
        "id": "4ibdtWlee2gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def sum_kernel(a, b, c):\n",
        "  position = cuda.grid(1)\n",
        "  if position < a.size:\n",
        "    c[position] = a[position] + b[position]\n",
        "\n",
        "VECTOR_SIZE = 321\n",
        "a = np.ones(VECTOR_SIZE)\n",
        "b = np.ones(VECTOR_SIZE)\n",
        "c = np.zeros(VECTOR_SIZE)\n",
        "\n",
        "# Copiando al device\n",
        "device_a = cuda.to_device(a)\n",
        "device_b = cuda.to_device(b)\n",
        "#device_c = cuda.device_array_like(c)\n",
        "device_c = cuda.device_array(VECTOR_SIZE)\n",
        "\n",
        "\n",
        "threads_per_block = 32\n",
        "my_grid = math.ceil(a.size / threads_per_block)\n",
        "sum_kernel[my_grid, threads_per_block](device_a, device_b, device_c)\n",
        "\n",
        "print(device_c.copy_to_host())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haESVYdyAp2T",
        "outputId": "1ae4e665-c257-4a44-be97-76a7fec4c353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 11 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suma de matrices con grid de una sola dimensión\n",
        "## Rejilla asociada a los renglones"
      ],
      "metadata": {
        "id": "m-RApwyLdZP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "@cuda.jit\n",
        "def addition_matrix_kernel(a, b, c):\n",
        "  i = cuda.grid(1)\n",
        "  if i < a.shape[0]:\n",
        "    for j in range(a.shape[1]):\n",
        "      c[i][j] = a[i][j] + b[i][j]\n",
        "\n",
        "ROWS = 64\n",
        "COLUMNS = 128\n",
        "host_a = np.arange(ROWS*COLUMNS).reshape((ROWS, COLUMNS))\n",
        "host_b = np.arange(ROWS*COLUMNS).reshape((ROWS, COLUMNS))\n",
        "host_c = np.zeros(shape=(ROWS, COLUMNS))\n",
        "\n",
        "dev_a = cuda.to_device(host_a)\n",
        "dev_b = cuda.to_device(host_b)\n",
        "dev_c = cuda.device_array_like(host_c)\n",
        "\n",
        "threads_per_block = 32\n",
        "my_grid = math.ceil(ROWS / threads_per_block)\n",
        "print(my_grid, threads_per_block)\n",
        "addition_matrix_kernel[my_grid, threads_per_block](dev_a, dev_b, dev_c)\n",
        "print(dev_c.copy_to_host())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sOsqf6OIr-K",
        "outputId": "e035105b-5b55-4a4e-844f-0e29cd3c73d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 32\n",
            "[[    0.     2.     4. ...   250.   252.   254.]\n",
            " [  256.   258.   260. ...   506.   508.   510.]\n",
            " [  512.   514.   516. ...   762.   764.   766.]\n",
            " ...\n",
            " [15616. 15618. 15620. ... 15866. 15868. 15870.]\n",
            " [15872. 15874. 15876. ... 16122. 16124. 16126.]\n",
            " [16128. 16130. 16132. ... 16378. 16380. 16382.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suma de matrices con grid de una sola dimensión\n",
        "## Rejilla asociada a las columnas"
      ],
      "metadata": {
        "id": "N8-dma0vdaYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "@cuda.jit\n",
        "def addition_matrix_kernel(a, b, c):\n",
        "  j = cuda.grid(1)\n",
        "  if j < a.shape[1]:\n",
        "    for i in range(a.shape[0]):\n",
        "      c[i][j] = a[i][j] + b[i][j]\n",
        "\n",
        "ROWS = 64\n",
        "COLUMNS = 129\n",
        "host_a = np.arange(ROWS*COLUMNS).reshape((ROWS, COLUMNS))\n",
        "host_b = np.arange(ROWS*COLUMNS).reshape((ROWS, COLUMNS))\n",
        "host_c = np.zeros(shape=(ROWS, COLUMNS))\n",
        "\n",
        "dev_a = cuda.to_device(host_a)\n",
        "dev_b = cuda.to_device(host_b)\n",
        "dev_c = cuda.device_array_like(host_c)\n",
        "\n",
        "threads_per_block = 32\n",
        "my_grid = math.ceil(COLUMNS / threads_per_block)\n",
        "print(my_grid, threads_per_block)\n",
        "addition_matrix_kernel[my_grid, threads_per_block](dev_a, dev_b, dev_c)\n",
        "print(dev_c.copy_to_host())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsJ__PgtOkoH",
        "outputId": "efdf0e85-e59d-4ce1-c09b-a85fc0f71e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 5 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    0.     2.     4. ...   252.   254.   256.]\n",
            " [  258.   260.   262. ...   510.   512.   514.]\n",
            " [  516.   518.   520. ...   768.   770.   772.]\n",
            " ...\n",
            " [15738. 15740. 15742. ... 15990. 15992. 15994.]\n",
            " [15996. 15998. 16000. ... 16248. 16250. 16252.]\n",
            " [16254. 16256. 16258. ... 16506. 16508. 16510.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suma de Matrices"
      ],
      "metadata": {
        "id": "3Wf8ceQvU_DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def matrix_addition(A, B, C):\n",
        "  row = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
        "  col = cuda.threadIdx.y + cuda.blockIdx.y * cuda.blockDim.y\n",
        "  #row, col = cuda.grid(2)\n",
        "  if row < C.shape[0] and col < C.shape[1]:\n",
        "    C[row][col] = A[row][col] + B[row][col]\n",
        "\n",
        "ROWS = 320\n",
        "COLUMNS = 1280\n",
        "host_a = np.full((ROWS, COLUMNS), 2, float)\n",
        "host_b = np.full((ROWS, COLUMNS), 4, float)\n",
        "\n",
        "dev_a = cuda.to_device(host_a)\n",
        "dev_b = cuda.to_device(host_b)\n",
        "dev_c = cuda.device_array((ROWS, COLUMNS))\n",
        "\n",
        "# 16 * 32 = 512 threads\n",
        "threads_per_block = (16, 32)\n",
        "blocks_x = math.ceil(ROWS / threads_per_block[0])\n",
        "blocks_y = math.ceil(COLUMNS / threads_per_block[1])\n",
        "grid = (blocks_x, blocks_y)\n",
        "print(grid)\n",
        "matrix_addition[grid, threads_per_block](dev_a, dev_b, dev_c)\n",
        "\n",
        "print(dev_c.copy_to_host())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTg3_5B2Poi2",
        "outputId": "f5ba657a-49c3-4dff-9cf8-8cbe07c21d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 40)\n",
            "[[6. 6. 6. ... 6. 6. 6.]\n",
            " [6. 6. 6. ... 6. 6. 6.]\n",
            " [6. 6. 6. ... 6. 6. 6.]\n",
            " ...\n",
            " [6. 6. 6. ... 6. 6. 6.]\n",
            " [6. 6. 6. ... 6. 6. 6.]\n",
            " [6. 6. 6. ... 6. 6. 6.]]\n"
          ]
        }
      ]
    }
  ]
}